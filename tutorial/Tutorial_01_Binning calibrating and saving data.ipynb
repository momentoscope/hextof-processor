{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEXTOF-processor binning procedure tutorial\n",
    "\n",
    "This notebook is an example of how to use the hextof-processor to process raw data from the FLASH DAQ raw data format, into binned arrays. These can then be used for analysis with the users favourite tools.\n",
    "\n",
    "The procedure described here is divided in two sections. First, a direct binning in the raw data axis is presented. The second includes calibration of Binding Energy and pump-probe time delay axes and allows binning in the desired physical quantities.\n",
    "\n",
    "This guide is intended to be used with the example raw data provided in this repository, and should be run from its original location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "# import numeric packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# invoke interactive matplotlib plots\n",
    "%matplotlib notebook \n",
    "\n",
    "# hextof-processor imports\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + '/src/') # add hextof-processor to path\n",
    "from processor.DataframeCreator import DataframeCreator\n",
    "from processor.utilities import calibration, diagnostics, misc, io, vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read raw data and generate single event dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using settings from tutorial.ini\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/asap3/flash/gpfs/pg2/2020/data/11008860/processed/parquet/per_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7d10d2c290e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataframeCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.yml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunNumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22097\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tutorial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read the raw data and generate the single event tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# prc.storeDataframes() # store the single event tables as parquet files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hextof-processor/src/processor/DldFlashDataframeCreatorExpress.py\u001b[0m in \u001b[0;36mreadData\u001b[0;34m(self, runs, ignore_missing_runs, settings, channels, beamtime_dir, parquet_path, beamtime_id, year, daq)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_parquet_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_PARQUET_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'per_file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_parquet_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_parquet_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Prepare a list of names for the files to read and parquets to write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/asap3/flash/gpfs/pg2/2020/data/11008860/processed/parquet/per_file'"
     ]
    }
   ],
   "source": [
    "# load the correct configuration for the dataset you want to read.\n",
    "# This can be different for each beamtime.\n",
    "prc = DataframeCreator('config.yml', runNumber=22097, settings = 'tutorial')\n",
    "\n",
    "prc.prc.readData() # read the raw data and generate the single event tables\n",
    "# prc.storeDataframes() # store the single event tables as parquet files.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DldFlashProcessor.\n",
       "- Run Number: 22097\n",
       "- MacrobunchIds: [None, None]\n",
       "- Settings loaded from /home/zains/hextof-processor/settings/tutorial.ini\n",
       "- Bins:"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prc.prc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the prc object can be re-created using the fresh parquet dataset only. This improves memory efficiency of the overall code. However, generating the parquet intermediate format is not mandatory, and binning can be performed, on small runs, also on the raw data loaded in memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **OPTIONAL** clear memory and load the single event tables from parquet:\n",
    "prc = DldFlashProcessor() # overwrite the processor with a fresh new instance\n",
    "prc.runNumber = 22097\n",
    "prc.DATA_PARQUET_DIR = '../tutorial/parquet' # manually overwrite parquet data folder. it is STRONGLY recomendend to use an SSD drive here.\n",
    "prc.readDataframes() # load data from single event tables saved as parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 define binning range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time of flight parameters\n",
    "tof_from = 620#180\n",
    "tof_to = 660\n",
    "tof_step = 0.003 # the bigger number between this and the original step size is used in the final binning size definition.\n",
    "\n",
    "# pump-probe delay parameters\n",
    "delay_from = -517\n",
    "delay_to = -512.5\n",
    "delay_step = 0.05\n",
    "\n",
    "# detector position parameters, valid both in x and y directions\n",
    "dld_from=0\n",
    "dld_to=3000\n",
    "dld_step=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 add binning parameters to the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc.resetBins() # ensure there is no previous binning axes assigned.\n",
    "prc.addBinning('dldTime', tof_from,tof_to,tof_step);\n",
    "prc.addBinning('delayStage',delay_from,delay_to,delay_step);\n",
    "# prc.addBinning('dldPosX',dld_from,dld_to,dld_step);\n",
    "# prc.addBinning('dldPosY',dld_from,dld_to,dld_step);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 compute binned arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prc.computeBinnedData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binning on calibrated axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 calibrate energy axis\n",
    "Create a column **Energy** in the dataframe, with the values of binding energy in eV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc.calibrateEnergy(toffset=355.,eoffset=113.29,l=0.8, # parameters defined by microscope settings. See documentation for details.\n",
    "    applyJitter=True,jitterAmplitude=1,jitterType='normal')# jitter parameters used to remove artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 calibrate pumpProbe Time\n",
    "Create a **pumpProbeTime** axis in the dataframe, by transforming stage delay values to the pump-probe corrected axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc.calibratePumpProbeTime(t0=-514.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 bin the data on the corrected axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc.resetBins()\n",
    "prc.addBinning('energy',-40,2,0.01);\n",
    "prc.addBinning('pumpProbeTime',-14,5,0.1);\n",
    "result = prc.computeBinnedData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "result.plot(cmap='terrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3b bin the data and normalize to the delay stage histogram\n",
    "The sweeping mode of the delay stage features long stops on the edges of the scanning range, which lead to artifacts in the number of counts as function of delay stage. In order to correct for this, it is possible to bin additionally on the delay stage and normalise to this axis.\n",
    "\n",
    "***ATTENTION: this is can be computationally heavy, since it requires an additional axis on which to bin. Might fail on smaller machines.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc.resetBins()\n",
    "energy_bin_parameters = -40, 2, 0.01 # from,to and step, in eV\n",
    "time_bin_parameters = -15,6.5, 0.05 # from,to and step, in ps\n",
    "ds_min,ds_max = dask.compute(prc.dd['delayStage'].min(),prc.dd['delayStage'].max())# calculate the range of the delay Stage\n",
    "\n",
    "\n",
    "prc.addBinning('pumpProbeTime', *time_bin_parameters) # uncommenting this will allow the binning to be in time, but this cannot be performed on small memory computers.\n",
    "prc.addBinning('delayStage',ds_min,ds_max, time_bin_parameters[2]*4) # bin the full delay stage range\n",
    "prc.addBinning('energy', *energy_bin_parameters)\n",
    "\n",
    "result = prc.normalizeDelay(prc.computeBinnedData(), ax='delayStage', preserve_mean=True).sum('delayStage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "result.plot(cmap='terrain')\n",
    "edc = result.mean('pumpProbeTime')\n",
    "edc /= edc.max()\n",
    "plt.plot(edc.energy,10*edc+result.pumpProbeTime.min(),color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 normalize EDCs to emphasize pumpProbe effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "result_edcnorm = result.transpose('pumpProbeTime','energy')\n",
    "result_edcnorm.values = result_edcnorm.values / result_edcnorm.mean('energy').values[:,None]\n",
    "ext = result_edcnorm.energy[0],result_edcnorm.energy[-1],result_edcnorm.pumpProbeTime[0],result_edcnorm.pumpProbeTime[-1]\n",
    "plt.imshow(result_edcnorm,origin='lower',aspect='auto',cmap='terrain',extent=ext,clim=(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving binned data\n",
    "Binned data can be saved as .h5 or tiff stacks (up to 4D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.save_binned(result_edcnorm,file_name='run22097_tutorial',path='../tutorial/raw/',format='h5')\n",
    "io.save_binned(result_edcnorm,file_name='run22097_tutorial',path='../tutorial/raw/',format='tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49cf1b99b458a759a3d83c7dee9b6494dee31bdc0508aa200a4853f37a5874ba"
  },
  "kernelspec": {
   "display_name": "hextof-express",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
